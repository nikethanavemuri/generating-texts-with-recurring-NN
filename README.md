# generating-texts-with-recurring-NN

The project demonstrates how to create a text-generating model using Long Short-Term Memory (LSTM) neural networks in Python. I used TensorFlow, NumPy, and random libraries to train a model on Shakespeare's works, enabling it to produce text in a similar style. 

The project focuses on creating a text generation system using Long Short-Term Memory (LSTM) neural networks in Python. It involves training a model on a dataset of text, such as literary works, to learn patterns and generate similar text sequences. The key steps include:

Data Preprocessing: The text data is cleaned and transformed into a format suitable for the model, including converting characters into numerical representations.

Model Development: An LSTM-based recurrent neural network is constructed using Python's TensorFlow library to capture the sequential dependencies in the text.

Training: The model is trained on the dataset, learning to predict the next character in a sequence based on prior inputs.

Text Generation: After training, the model can generate new text that mimics the style and structure of the original dataset, producing creative outputs based on given input prompts.

The project provides a practical implementation of LSTM networks for sequence modeling tasks and showcases the potential of neural networks in creative applications like text generation.

The Sample Shakespear's text used can be found here: https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt

--- Keras Documentation ---
https://keras.io/api/

--- Tensorflow Documentation ---
https://www.tensorflow.org/api_docs
